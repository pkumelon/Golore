name: adam_winogrande_lora
project: zo_bench_winogrande_Llama-2-13b
command:
  - ${interpreter}
  - ${program}
  - ${args}
  - "--lora"
  - "--model_name=meta-llama/Llama-2-13b-hf"
  - "--task_name=WinoGrande"
  - "--output_dir=result/WinoGrande-ft-$TAG"
  - "--num_train_epochs=5"
  - "--per_device_train_batch_size=16"
  - "--load_best_model_at_end"
  - "--evaluation_strategy=steps"
  - "--save_strategy=steps"
  - "--save_total_limit=1"
  - "--eval_steps=500"
  - "--max_steps=20000"
  - "--logging_steps=10"
  - "--num_eval=1000"
  - "--num_train=1000"
  - "--num_dev=100"
  - "--train_as_classification=False"
  - "--perturbation_mode=two_side"
  - "--trainer=regular"
  - "--optimizer=adam"
  - "--train_set_seed=0"
  - "--lr_scheduler_type=constant"
  - "--save_steps=500"
  - "--load_float16"
method: grid
metric:
  goal: maximize
  name: test_acc
parameters:
  learning_rate:
    values:
      - 1e-2
      - 1e-3
      - 1e-4
      - 1e-5
      - 1e-6
  weight_decay:
    values:
      - 0

program: run.py